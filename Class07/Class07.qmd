---
title: "Class 7: Clustering and PCA"
author: "Jonathan Cifuentes (A17238145)"
format: html
---

## Clustering

First let's make up some data to cluster so we can get a feel for these methods and how to work with them.

We can use the `rnorm()` function to get random numbers from a normal distribution around a given `mean`.
```{r}
hist(rnorm(5000, mean=3))
```

Let's get 30 points with a mean of 3 and -3.
```{r}
a<- rnorm(30, mean=3)
b<- rnorm(30, mean=-3)
rnorm_vector<- c(b, a)
rnorm_vector
```

Put two of these together
```{r}
x<- cbind(rnorm_vector, y=rev(rnorm_vector))
x
plot(x)
```

## k-means clustering.

Very popular clustering method that we can use with the `kmeans()` function in base R
```{r}
km <- kmeans(x, centers=2)
km
```

```{r}
km$size
```

Use the `kmeans()` fucntion setting k to 2 and nstart=20
How many points are in each cluster?
30 points

What 'component' of your result object details
 - cluster size?
```{r}
km$size
```
 
 - cluster assignment/membership?
```{r}
km$cluster
```
 
 - cluster center?
```{r}
km$centers
```
 
 Plot x colored by the kmeans 
```{r}
mycols <- c(1,2)
plot(x, col=km$cluster)
#pch makes the square and cex changes the size
points(km$centers, col="blue", pch=15, cex=3)
```
> Q Let's cluster into 3 groups or same `x` data and make a plot.

```{r}
km <- kmeans(x, centers=3)
plot(x, col=km$cluster)
```

#hierarchial Clustering 

We can use the `hclust()` function for Hierarchial Clustering.
Unlike `kmeans()` where we could just pass in our data as input, we need to give `hclust()` a "distance matrix."

 We will use the `dist()` function to start with.
 
```{r}
d <- dist(x)
hc <- hclust(d)
hc
```
 
```{r}
#makes plot that has clusters with data points grouped to each data set
plot(hc)
```
 
 I can now `cutree()` my tree to yield a cluster membership vector.
```{r}
#h stands for height
grps <- cutree(hc, h=8)
grps
```
 
You can also tell `cutree()` to cut where it yields "k" groups.
```{r}
cutree(hc, k=2)
hc
```

```{r}
plot(x, col= grps)
```

#Principal Component Analysis (PCA)
Uk Food data import
```{r}
url <- "https://tinyurl.com/UK-foods"
z <- read.csv(url)
```
> Q How many rows and columns are in your new data frame named z? What R functions could you use to answer this questions?

```{r}
nrow(z)
ncol(z)
```
## Preview the first 6 rows
```{r}

head(z)
```
## fix columns since 4 show up instead of 5, countries are being confused as a columns
```{r}
# Note how the minus indexing works
rownames(z) <- z[,1]
z <- z[,-1]
head(z)
```

```{r}
dim(z)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer the second method with the `z <- z[,-1]` but it cannot be run multiple times or else it will continuously delete column info


**Spotting major differences and trends**
```{r}
barplot(as.matrix(z), beside=T, col=rainbow(nrow(z)))
```


>Q3: Changing what optional argument in the above barplot() function results in the following plot?

change beside to false
```{r}
barplot(as.matrix(z), beside=F, col=rainbow(nrow(z)),  )
```

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(z, col=rainbow(10), pch=16)
```

> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

**PCA to the Rescue**

```{r}
# Use the prcomp() PCA function 
pca <-prcomp( t(z) )
summary(pca)
```

```{r}
attributes(pca)
```

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.


```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500), col=c("orange","red","blue", "darkgreen"))
text(pca$x[,1], pca$x[,2], labels= colnames(z), col=c("orange","red","blue", "darkgreen"))
```

